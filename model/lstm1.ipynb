{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = str(1)\n",
    "#os.environ[\"TORCH_USE_CUDA_DSA\"]= str(0)\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTO_SW_LSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM class\n",
    "\n",
    "    Parameters:\n",
    "        window_size (int): Size of sliding window\n",
    "        hidden_size (int): Number of hidden nodes in LSTM\n",
    "        num_layers  (int): Number of LSTM layers\n",
    "        n_features  (int): Number of values at each timestep\n",
    "        stride      (int): Stride length, 0<stride<=window_size\n",
    "        bsize       (int): Batch size during training\n",
    "        device      (str): \"cuda\" or \"cpu\"\n",
    "        bidir       (bool): Bidirectional LSTM\n",
    "        nout        (list): List of integers [h1, h2, ..., hn, hout] for size of output DNN\n",
    "        dropout     (int): Dropout for LSTM\n",
    "        dropout2    (int): Dropout for output DNN\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,window_size,hidden_size,num_layers,n_features,stride,bsize,device,bidir,nout,dropout,dropout2):\n",
    "        super(MTO_SW_LSTM, self).__init__()\n",
    "        \n",
    "        # Initiate RNN\n",
    "        self.rnn = nn.LSTM(input_size=n_features*window_size, hidden_size=hidden_size,\n",
    "                           num_layers=num_layers, bidirectional=bidir, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        # Initiate output DNN\n",
    "        units = []\n",
    "        units.append(('fc0', nn.Linear(hidden_size, nout[0])))\n",
    "        for i in range(len(nout)-2):\n",
    "            units.append(('do'+str(i), nn.Dropout(dropout2)))\n",
    "            units.append(('af'+str(i), nn.Tanh()))\n",
    "            units.append(('lin'+str(i), nn.Linear(nout[i],nout[i+1])))\n",
    "        units.append(('do'+str(len(nout)), nn.Dropout(dropout2)))\n",
    "        units.append(('af'+str(len(nout)), nn.Tanh()))\n",
    "        units.append(('lin'+str(len(nout)), nn.Linear(nout[-2],nout[-1])))\n",
    "        self.dnn = nn.Sequential(OrderedDict(units))\n",
    "        # Choose activation function:\n",
    "        self.af = nn.Sigmoid()\n",
    "        \n",
    "        # Settings\n",
    "        self.bidir = bidir\n",
    "        self.stride = stride\n",
    "        self.ws = window_size\n",
    "        self.device = device\n",
    "        self.bsize = bsize\n",
    "        if bidir:\n",
    "            self.nb_lstm_layers = num_layers*2\n",
    "        else:\n",
    "            self.nb_lstm_layers = num_layers\n",
    "        self.nb_lstm_units = hidden_size\n",
    "        self.to(self.device)\n",
    "    \n",
    "    '''\n",
    "    Initialize hidden parameters in LSTM\n",
    "    \n",
    "    Parameters:\n",
    "        using_gpu (bool):\n",
    "        nseq      (int): Number of input sequences\n",
    "    '''\n",
    "    def init_hidden(self, using_gpu, nseq):\n",
    "        # the weights are of the form (nb_layers, batch_size, nb_lstm_units)\n",
    "        # Choose torch.ones, or torch.zeros or torch.randn:\n",
    "        hidden_a = torch.ones(self.nb_lstm_layers, nseq, self.nb_lstm_units)\n",
    "        hidden_b = torch.ones(self.nb_lstm_layers, nseq, self.nb_lstm_units)\n",
    "\n",
    "        if using_gpu:\n",
    "            hidden_a = hidden_a.cuda()\n",
    "            hidden_b = hidden_b.cuda()\n",
    "\n",
    "        hidden_a = Variable(hidden_a)\n",
    "        hidden_b = Variable(hidden_b)\n",
    "\n",
    "        return (hidden_a, hidden_b)\n",
    "\n",
    "    '''\n",
    "    Forward pass\n",
    "    \n",
    "    Parameters:\n",
    "        x           (list): List of sequence and label correspondences, [(X,y), ...], size(X)=(n_timesteps, n_features)\n",
    "        using_gpu   (bool):\n",
    "        nseq        (int): Number of input sequences\n",
    "    '''\n",
    "    def forward(self, x, using_gpu, nseq):\n",
    "        \n",
    "        self.hidden = self.init_hidden(using_gpu, nseq)\n",
    "        \n",
    "        lengths = [x_.size()[0] for x_ in x]\n",
    "        \n",
    "        maxlen = max(lengths)\n",
    "        for i in range(len(lengths)):\n",
    "            if (lengths[i]-self.ws)%self.stride==0:\n",
    "                lengths[i] = lengths[i]\n",
    "            else:\n",
    "                lengths[i] = lengths[i]+(self.stride-(lengths[i]-self.ws)%self.stride)\n",
    "        \n",
    "        x_padded = nn.utils.rnn.pad_sequence(x, batch_first=True)\n",
    "        toadd = self.stride-maxlen%self.stride\n",
    "        \n",
    "        if toadd<self.stride:\n",
    "            x_padded = torch.cat((x_padded,torch.zeros((x_padded.size()[0],toadd,x_padded.size()[2])).float().to(self.device)),dim=1)\n",
    "        \n",
    "        x_padded = torch.stack([torch.flatten(x_padded[:,i*self.stride:i*self.stride+self.ws,:],start_dim=1,end_dim=2) for i in range(np.int64((x_padded.size()[1]-self.ws)/self.stride+1))], dim=1)\n",
    "        newlengths = []\n",
    "        for i in range(len(lengths)):\n",
    "            if lengths[i]>=self.ws:\n",
    "                newlengths.append(np.int64((lengths[i]-self.ws)/self.stride+1))\n",
    "            else:\n",
    "                newlengths.append(1)\n",
    "        lengths = newlengths\n",
    "        b, s, n = x_padded.shape\n",
    "        \n",
    "        # pack padded sequence\n",
    "        x_padded = nn.utils.rnn.pack_padded_sequence(x_padded, lengths=lengths, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        out, self.hidden = self.rnn(x_padded, self.hidden)\n",
    "        \n",
    "        # unpack the feature vector\n",
    "        out, lens_unpacked = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        ndim = 1\n",
    "        if self.bidir:\n",
    "            ndim = 2\n",
    "        out = out.view(b, s, ndim, self.nb_lstm_units)\n",
    "        \n",
    "        # many-to-one rnn, get the last result\n",
    "        y = torch.stack([out[i,np.array(lengths[i])-1, -1, :] for i in range(len(lengths))],dim=0)\n",
    "        y = self.af(self.dnn(y))\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    '''\n",
    "    Train model\n",
    "    \n",
    "    Parameters:\n",
    "        train_data  (list): List of sequence and label correspondences, [(X,y), ...], size(X)=(n_timesteps, n_features)\n",
    "        val_data    (list): List of sequence and label correspondences, [(X,y), ...], size(X)=(n_timesteps, n_features)\n",
    "        epochs      (int):\n",
    "        bsize       (int): Batch size\n",
    "        learning_rate (float):\n",
    "        using_gpu   (bool):\n",
    "        testnbr     (int): How often to test on val_data\n",
    "    '''\n",
    "    def train_model(self, train_data, val_data, epochs, bsize, learning_rate, using_gpu, testnbr):\n",
    "        nseq = len(train_data)\n",
    "        \n",
    "        optimizer = optim.RMSprop(self.parameters(), lr=learning_rate)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        time0 = time.time()\n",
    "        running_loss_list= []\n",
    "        val_loss_list = []\n",
    "                \n",
    "        for ee in range(epochs):\n",
    "            #print(\"Running epoch \", ee+1)\n",
    "            \n",
    "            idxs = list(range(nseq))\n",
    "            random.shuffle(idxs)\n",
    "            \n",
    "            self.train()\n",
    "            \n",
    "            # defining gradient in each epoch as 0\n",
    "            if using_gpu:\n",
    "                for param in self.parameters():\n",
    "                    param.grad = None\n",
    "            else:\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            running_loss = 0\n",
    "            \n",
    "            for bb in range(np.int64(np.floor(nseq/bsize))):\n",
    "                \n",
    "                x_train = [train_data[i][0] for i in idxs[bb*bsize:(bb+1)*bsize]]\n",
    "                y_train = torch.cat([train_data[i][1] for i in idxs[bb*bsize:(bb+1)*bsize]],dim=0)\n",
    "                \n",
    "                nbatchseq = len(x_train)\n",
    "                \n",
    "                out = self.forward(x_train,using_gpu,nbatchseq)\n",
    "                \n",
    "                loss = criterion(out, y_train)\n",
    "                \n",
    "                loss.backward()\n",
    "                \n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            # Calculate losses and print epoch information\n",
    "            if (ee+1)%testnbr==0 or ee==0:\n",
    "                self.eval()\n",
    "                x_val = [obj[0] for obj in val_data]\n",
    "                y_val = torch.cat([obj[1] for obj in val_data],dim=0)\n",
    "                mean_train_loss = running_loss/np.floor(nseq/bsize)\n",
    "                running_loss_list.append(mean_train_loss)\n",
    "                with torch.no_grad():\n",
    "                    nvalseq = len(x_val)\n",
    "                    out = self.forward(x_val,using_gpu,nvalseq)\n",
    "                    loss = criterion(out, y_val)\n",
    "                    val_loss_list.append(loss.item())\n",
    "                print(\"Epoch {} - Training loss: {} - Validation loss: {}\".format(ee+1, mean_train_loss, val_loss_list[-1]))\n",
    "        print(\"Finished training in \", time.time()-time0, \" seconds\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MTO_SW_LSTM(\n",
      "  (rnn): LSTM(200, 100, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n",
      "  (dnn): Sequential(\n",
      "    (fc0): Linear(in_features=100, out_features=50, bias=True)\n",
      "    (do0): Dropout(p=0.2, inplace=False)\n",
      "    (af0): Tanh()\n",
      "    (lin0): Linear(in_features=50, out_features=100, bias=True)\n",
      "    (do3): Dropout(p=0.2, inplace=False)\n",
      "    (af3): Tanh()\n",
      "    (lin3): Linear(in_features=100, out_features=10, bias=True)\n",
      "  )\n",
      "  (af): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "window_size = 20\n",
    "hidden_size = 100\n",
    "num_layers = 2\n",
    "n_features = 10\n",
    "stride = 10\n",
    "bsize = 10\n",
    "device = 'cuda'\n",
    "bidir = True\n",
    "nout = [50, 100, 10]\n",
    "dropout = 0.1\n",
    "dropout2 = 0.2\n",
    "\n",
    "rnn = MTO_SW_LSTM(window_size,hidden_size,num_layers,n_features,stride,bsize,device,bidir,nout,dropout,dropout2)\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "train_data = []\n",
    "val_data = []\n",
    "for i in range(100):\n",
    "    X_seq = torch.randn((500,n_features)).float().to(device)\n",
    "    y_seq = torch.randn((1,nout[-1])).float().to(device)\n",
    "    train_data.append((X_seq,y_seq))\n",
    "    X_seq = torch.randn((500,n_features)).float().to(device)\n",
    "    y_seq = torch.randn((1,nout[-1])).float().to(device)\n",
    "    val_data.append((X_seq,y_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "learning_rate = 0.1\n",
    "using_gpu = True\n",
    "testnbr = 10\n",
    "rnn.train_model(train_data, val_data, epochs, bsize, learning_rate, using_gpu, testnbr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
